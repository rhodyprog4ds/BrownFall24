---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.16.4
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

# Fixing Values
o far, we've dealt with structural issues in data. but there's a lot more to
cleaning.  

Today,  we'll deal with how to fix the values within  the data.
## Cleaning Data review

Instead of more practice with these manipulations, below are more
examples of cleaning data to see how these types of manipulations get used.  
Your goal here is not to memorize every possible thing, but to build a general
idea of what good data looks like and good habits for cleaning data and keeping
it reproducible.  
- [Cleaning the Adult Dataset](https://ryanwingate.com/projects/machine-learning-data-prep/adult/adult-cleaning/)
- [All Shades](https://github.com/the-pudding/data/tree/master/foundation-names#allshadescsv--allshadesr)
Also here are some tips on general data management and organization.

This article is a comprehensive [discussion of data cleaning](https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4).

### A Cleaning Data Recipe

__not everything possible, but good enough for this course__


1. Can you use parameters to read the data in better?
1. Fix the index and column headers (making these easier to use makes the rest easier)
1. Is the data strucutred well?
1. Are there missing values?
1. Do the datatypes match what you expect by looking at the head or a sample?
1. Are categorical variables represented in usable way?
1. Does your analysis require filtering or augmenting the data?



## What is clean enough?

This is a great question, without an easy answer.

It depends on what you want to do.  This is why it's important to have potential
questions in mind if you are cleaning data for others *and* why we often have to
do a little bit more preparation after a dataset has been "cleaned"



Dealing with missing data is a whole research area. There isn't one solution.


[in 2020 there was a whole workshop on missing](https://artemiss-workshop.github.io/)

one organizer is the main developer of [sci-kit learn](https://scikit-learn.org/stable/) the ML package we will use soon.  In a [2020 invited talk](https://static.sched.com/hosted_files/ray2020/08/Keynote-%20Easier%20Machine%20Learning%20Thoughts%20From%20Scikit-Learn%20-%20Ga%C3%ABl%20Varoquaux%2C%20Research%20Director%2C%20Inria.pdf) he listed more automatic handling as an active area of research  and a development goal for sklearn.

There are also many classic approaches both when training and when [applying models](https://www.jmlr.org/papers/volume8/saar-tsechansky07a/saar-tsechansky07a.pdf).

[example application in breast cancer detection](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.701.4234&rep=rep1&type=pdf)

Even in pandas, dealing with [missing values](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html) is under [experimentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html#missing-data-na)
 as to how to represent it symbolically


Missing values even causes the [datatypes to change](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html#missing-data-casting-rules-and-indexing)


That said, there are are om
Pandas gives a few basic tools:

- dropna
- fillna




Filling can be good if you know how to fill reasonably, but don't have data to
spare by dropping.  For example
- you can approximate with another column
- you can approximate with that column from other rows

Special case, what if we're filling a summary table?
- filling with a symbol for printing can be a good choice, but not for analysis.

**whatever you do, document it**

```{code-cell} ipython3
import pandas as pd
import seaborn as sns
import numpy as np 
na_toy_df = pd.DataFrame(data = [[1,3,4,5],[2 ,6, np.nan],[np.nan]*4,[np.nan,3,4,5]], columns=['a','b','c','d'])

# make plots look nicer and increase font size
sns.set_theme(font_scale=2)

# todays data
arabica_data_url = 'https://raw.githubusercontent.com/jldbc/coffee-quality-database/master/data/arabica_data_cleaned.csv'

coffee_df = pd.read_csv(arabica_data_url)
```

## Missing values


We tend to store missing values as `NaN` or use the constants:

```{code-cell} ipython3
pd.NA, np.nan
```

Pandas makes that a special typed object, but converts the whole *column* to float

Numpy uses *float* value for `NaN` that is defined by [IEEE floating point standard](https://en.wikipedia.org/wiki/IEEE_754)


```{code-cell} ipython3
type(pd.NA),type(np.nan)
```


````{admonition} Floats are weird

![float comic for jvns.ca](https://wizardzines.com/zines/integers-floats/samples/2-floating-point.png)

there are values that cannot be represented. 

- [see float.exposed for infinity ](https://float.exposed/0x7ff0000000000000)
-  [negative infinity](https://float.exposed/0xfff0000000000000)
- [see a nan](https://float.exposed/0x7ff8000000000000) (which bits can be changd without making it *not* nan)
- from [9007199254740992.0](https://float.exposed/0x4340000000000000)  the next closest value is [9007199254740994.0](https://float.exposed/0x4340000000000001)... no values in between can be stores in double precision float

````

We can see a few in this toy dataset 
:::::{margin}
:::{warning}
I simplified here relative to class, by using the bigger table I made on the fly above. 
:::::

This data has some missing values 
```{code-cell} ipython3
na_toy_df
```

Let's try the default behavior of `dropna`

```{code-cell} ipython3
na_toy_df.dropna()
```
This is the same as
```{code-cell} ipython3
na_toy_df.dropna(how='any',subset=na_toy_df.columns,axis=0)
```


by default it drops all of the *rows* where **any** of the elements are missing (1 or more)
we can change `how` to its other mode: 
```{code-cell} ipython3
na_toy_df.dropna(how='all')
```

in `'all'` mode it only drops rows where **all** of the values are missing

we can also change it to work along columns (`axis=1`) instead

```{code-cell} ipython3
na_toy_df.dropna(how='all',axis=1)
```

None of the columns are *all* missing so nothing is dropped

Let's say we had an analysis where we neded at least one of column `c` or `d` or else we could not
use the row, we can check that this way: 
```{code-cell} ipython3
na_toy_df.dropna(how='all',subset=['c','d']
```

### Filling missing values 

Let's look at a real dataset now
```{code-cell} ipython3
coffee_df.info()
```

The 'Lot.Number' has a lot of NaN values, how can we explore it?

We can look at the type:

```{code-cell} ipython3
coffee_df['Lot.Number'].dtype
```
And we can look at the value counts.
```{code-cell} ipython3
coffee_df['Lot.Number'].value_counts()
```


We see that a lot are '1', maybe we know that when the data was collected, if the Farm only has one lot, some people recorded '1' and others left it as missing. So we could fill in with 1:

```{code-cell} ipython3
coffee_df['Lot.Number'].fillna('1').head()
```

```{code-cell} ipython3
coffee_df['Lot.Number'].head()
```


Note that even after we called `fillna` we display it again and the original data is unchanged.
To save the filled in column, *technically* we have a few choices:
- {fa}`fa-ban` use the `inplace` parameter. This doesn't offer performance advantages, but does It still copies the object, but then reassigns the pointer. Its under discussion to [deprecate](https://github.com/pandas-dev/pandas/issues/16529)
- {fa}`fa-check` write to a new DataFrame
- {fa}`fa-check` add a column


we will add a column

```{code-cell} ipython3
coffee_df['lot_number_clean'] = coffee_df['Lot.Number'].fillna('1')
```

```{code-cell} ipython3
coffee_df.head(1)
```

```{code-cell} ipython3
coffee_df.shape
```


## Dropping


Dropping is a good choice when you otherwise have a lot of data and the data is
missing at random.

Dropping can be risky if it's not missing at random. For example, if we saw in
the coffee data that one of the scores was missing for all of the rows from one
country, or even just missing more often in one country, that could bias our
results.  

here will will focus on how this impacts how much data we have: 

```{code-cell} ipython3
coffee_df.dropna().shape
```

we lose a lot this way.

We could instead tell it to only drop rows with `NaN` in a subset of the columns.

```{code-cell} ipython3
coffee_df.dropna(subset=['altitude_low_meters']).shape
```


Now, it drops any row with one or more `NaN` values in that column.

In the [Open Policing Project Data Summary](https://openpolicing.stanford.edu/data/) we saw that they made a summary information that showed which variables had at least 70% not missing values.  We can similarly choose to keep only variables that have more than a specific threshold of data, using the `thresh` parameter and `axis=1` to drop along columns.

```{code-cell} ipython3
n_rows, _ = coffee_df.shape
coffee_df.dropna(thresh=.7*n_rows, axis=1).shape
```


## Inconsistent values



This was one of the things that many of you anticipated or had observed.  A useful way to investigate for this, is to use `value_counts` and sort them alphabetically by the values from the original data, so that similar ones will be consecutive in the list. Once we have the `value_counts()` Series, the values from the `coffee_df` become the index, so we use `sort_index`.

Let's look at the `in_country_partner` column


```{code-cell} ipython3
:tags: ["scroll-output"]
coffee_df['In.Country.Partner'].value_counts().sort_index()
```



We can see there's only one `Blossom Valley International\n` but 58 `Blossom Valley International`, the former is likely a typo, especially since `\n` is a special character for a newline. Similarly, with 'Specialty Coffee Ass' and 'Specialty Coffee Association'.

```{code-cell} ipython3
:tags: ["scroll-output"]
partner_corrections = {'Blossom Valley International\n':'Blossom Valley International',
  'Specialty Coffee Ass':'Specialty Coffee Association'}
coffee_df['in_country_partner_clean'] = coffee_df['In.Country.Partner'].replace(
  to_replace=partner_corrections)
coffee_df['in_country_partner_clean'].value_counts().sort_index()
```



## Multiple values in a single column

Let's look at the column about the bag weights

```{code-cell} ipython3
coffee_df['Bag.Weight'].head()
```
it has both the *value* and the *units* in a single column, which is not what we want. 

This would be better in two separate columns

```{code-cell} ipython3
bag_df = coffee_df['Bag.Weight'].str.split(' ').apply(pd.Series).rename({0:'bag_weight_clean',
                                                                1:'bag_weight_unit'},
                                                              axis=1)
bag_df.head()
```

This:
- picks the column
- treats it as a string with the [pandas Series attribute `.str`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.html)
- uses [base python `str.split` ](https://docs.python.org/3/library/stdtypes.html#str.split)to split at `' '` spaces and makes a list
- casts each list to Series with `.apply(pd.Series)` 
- renames the resulting columns from being numbered to usable names `rename({0:'bag_weight_clean', 1:'bag_weight_unit'}, axis=1)`

````{tip}
The `.apply(pd.Series)` works on dictionaries too (anything hte [series constructor](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) can take to its `data` parameter) so this
is good for json data
````

The following subsections break down the casting and string methods in more detail
### String methods

Python has a powerful [string class](https://docs.python.org/3/library/stdtypes.html#string-methods). There is also an even more powerful [`string` module](https://docs.python.org/3/library/string.html)


we only need the base `str` methods most of the time
```{code-cell} ipython3
example_str = 'kjksfjds sklfjsdl'
type(example_str)
```

Some helpful ones: 

```{code-cell} ipython3
example_str.split()
```

this gives a list

you can also change the separator

```{code-cell} ipython3
'phrases-with-hyphens'.split('-')
```

there are also mehtods for chaning the case and other similar things. **Use these* instead of implementing your own string operations!! 

```{code-cell} ipython3
example_str.upper(), example_str.capitalize()
```




### Casting Review

If we have a variable that is not the type we want like this:
```{code-cell} ipython3
a='5'
```


we can check type
```{code-cell} ipython3
type(a)
```

and we can use the name of the type we want, as a function to *cast* it to the new type. 

```{code-cell} ipython3
int(5)
```

and check 
```{code-cell} ipython3
type(int(a))
```

## Combining parts of dataframes

```{code-cell} ipython3
bag_df.head()
```

we can pass `pd.concat` and iterable of pandas objects (here a `list` of `DataFrames`) and it will, by default stack them vertically, or with `axis=1` stack the horizontally

```{code-cell} ipython3
pd.concat([coffee_df,bag_df],axis=1)
```

(quantize)=
## Quantizing a variable

Sometimes a variable is recorded continous, or close (like age in years, technically integers are discrete, but for wide enough range it is not very categorical) but we want to analyze it as if it is categorical.  

We can add a new variable that is *calculated* from the original one.  

Let's say we want to categorize coffes as small, medium or large batch size based on
the quantiles for the `'Number.of.Bags'` column. 

First, we get an idea of the distribution with EDA to make our plan: 
```{code-cell} ipython3
coffee_df_bags['Number.of.Bags'].describe()
```


```{code-cell} ipython3
coffee_df_bags['Number.of.Bags'].hist()
```
We see that most are small, but there is at least one major outlier, 75% are below 275, but the max is 1062. 



We can use `pd.cut` to make discrete values 

```{code-cell} ipython3
pd.cut(coffee_df_bags['Number.of.Bags'],bins=3).sample(10)
```



by default, it makes bins of equal size, meaning the range of values. This is not good based on what we noted above. Most will be in one label

```{code-cell} ipython3
pd.cut(coffee_df_bags['Number.of.Bags'],bins=3).hist()
```

TO make it better, we can specify the bin edges instead of only the number

```{code-cell} ipython3
min_bags = coffee_df_bags['Number.of.Bags'].min()
sm_cutoff = coffee_df_bags['Number.of.Bags'].quantile(.33)
md_cutoff = coffee_df_bags['Number.of.Bags'].quantile(.66)
max_bags = coffee_df_bags['Number.of.Bags'].max()
pd.cut(coffee_df_bags['Number.of.Bags'],
        bins=[min_bags,sm_cutoff,md_cutoff,max_bags]).head()
```

here, we made cutoffs individually and pass them as a list to `pd.cut`

This is okay for 3 bins, but if we change our mind, it's a lot of work to make more. 
Better is to make the bins more programmatically: 

```{code-cell} ipython3
[coffee_df_bags['Number.of.Bags'].quantile(pct) for pct in np.linspace(0,1,4)]
```

`np.linspace` returns a numpyarray of evenly (linearly; there is also logspace) spaced
numbers. From the start to the end value for the number you specify. Here we said 4 evenly spaced from 0 to 1. 

this is the same as we had before (up to rounding error)

```{code-cell} ipython3
[min_bags,sm_cutoff,md_cutoff,max_bags]
```

Now we can use these and optionally, change to text labels (which then means we have to update that too if we change the number 4 to another number, but still less work than above)

```{code-cell} ipython3
bag_num_bins = [coffee_df_bags['Number.of.Bags'].quantile(pct) for pct in np.linspace(0,1,4)]
pd.cut(coffee_df_bags['Number.of.Bags'],
        bins=bag_num_bins,labels = ['small','medium','large']).head()
```

we could then add this to the dataframe to work with it
```{code-cell} ipython3

```

```{code-cell} ipython3

```


```{code-cell} ipython3

```

```{code-cell} ipython3

```

## Questions

### How can I rename without a dicionary

Really, best practice is a dictionary or function, that is what[ `rename`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html) uses. 

You can assign to the columns attribute, but then you have to provide all of the column names

### Why are strings `object`?

it's largely [for backwards compatibility](https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html)