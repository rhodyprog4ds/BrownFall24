---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.16.4
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

# More regression

```{code-cell} ipython3
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

import pandas as pd
sns.set_theme(font_scale=2,palette='colorblind')
```

## Multivariate Regression

We can also load data from Scikit learn.

This dataset includes 10 features measured on a given date and an measure of
diabetes disease progression measured one year later. The predictor we can train
with this data might be someting a doctor uses to calculate a patient's risk.  


```{code-cell} ipython3
# Load the diabetes dataset
diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)
X_train,X_test, y_train,y_test = train_test_split(diabetes_X, diabetes_y ,
                                                  test_size=20,random_state=0)
```
by default it would have returned a very different object
```{code-cell} ipython3
db_bunch = datasets.load_diabetes()
```

```{code-cell} ipython3
type(db_bunch)
```

it has a very useful attribute, the data description:
```{code-cell} ipython3

print(db_bunch.DESCR)
```


This model predicts what lab measure a patient will have one year in the future
based on lab measures in a given day.  Since we see that this is not a very high
r2, we can say that this is not a perfect predictor, but a Doctor, who better
understands the score would have to help interpret the core.


```{code-cell} ipython3
regr_db = linear_model.LinearRegression()
regr_db.fit(X_train, y_train)
```

We can look at the estimator again and see what it learned. It describes the model like a line:

$$ \hat{y} = mx+b$$

except in this case it's multivariate, so we can write it like:

$$ \hat{y} = \beta^Tx + \beta_0 $$

where $\beta$ is the `regr_db.coef_` and $\beta_0$ is `regr_db.intercept_` and that's a vector multiplication and $\hat{y}$ is `y_pred` and $y$ is `y_test`.  

In scalar form it can be written like

$$ \hat{y} = \sum_{k=0}^d(x_k*\beta_k) + \beta_0$$

where there are $d$ features, that is $d$= `len(X_test[k])` and $k$ indexed into it. For example in the below $k=0$


```{code-cell} ipython3
y_pred = regr_db.predict(X_test)
r2_score(y_test,y_pred)
```

```{code-cell} ipython3
np.sqrt(mean_squared_error(y_test,y_pred))
```

```{code-cell} ipython3
np.mean(y_test)
```

```{code-cell} ipython3
np.std(y_test)
```

```{code-cell} ipython3
X_train.shape
```

```{code-cell} ipython3
lasso = linear_model.Lasso()
lasso.fit(X_train, y_train)
lasso.score(X_test, y_test)
```

```{code-cell} ipython3
lasso.coef_
```

```{code-cell} ipython3
lasso = linear_model.Lasso(.25)
lasso.fit(X_train, y_train)
lasso.score(X_test, y_test)
```

```{code-cell} ipython3
lasso.coef_
```

```{code-cell} ipython3
from sklearn.preprocessing import PolynomialFeatures
```

```{code-cell} ipython3
pol = PolynomialFeatures()
```

```{code-cell} ipython3
X_train2 = pol.fit_transform(X_train)
X_test2 = pol.fit_transform(X_test)
```

```{code-cell} ipython3
X_train2.shape
```

```{code-cell} ipython3
10+ 10+ sum(range(10)) +1
```

```{code-cell} ipython3
lasso2 = linear_model.Lasso(.27)
lasso2.fit(X_train2, y_train)
lasso2.score(X_test2, y_test)
```

```{code-cell} ipython3
lasso2.coef_
```

```{code-cell} ipython3

```
