---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.16.4
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

# Task Review and Cross Validation

## ML Tasks

We learned classification first, because it shares similarities with each
regression and clustering, while regression and clustering have less in common.

- Classification is supervised learning for a categorical target.  
- Regression is supervised learning for a continuous target.
- Clustering is unsupervised learning for a categorical target.


We have used a small flowchart for the tasks:
```{mermaid}
flowchart TD
    labels{Do you have labels <br>in the dataset? }
    labels --> |yes| sup[Supervised Learning]
    labels --> |no| unsup[Unsupervised Learning]
    sup --> ltype{What type of variable <br> are the labels?}
    ltype --> |continuous| reg[Regression]
    ltype --> |discrete| clas[Classification]
    unsup --> groups{Do you think there are <br> groups within the data?}
    groups --> |yes | clus[Clutering]
```

Sklearn provides a nice [flow chart](https://scikit-learn.org/stable/machine_learning_map.html) for thinking through both tasks (big blocks) and models (the small green boxes) 

![estimator flow chart](../img/ml_map.svg)

Predicting a category is another way of saying categorical target. Predicting a
quantitiy is another way of saying continuous target. Having lables or not is
the difference between


The flowchart assumes you know what you want to do with data and that is the
ideal scenario. You have a dataset and you have a goal.
For the purpose of getting to practice with a variety of things, in this course
we ask you to start with a task and then find a dataset. Assignment 9 is the
last time that's true however. Starting with Assignment 10, you can choose and focus on a specific application domain and then
choose the right task from there.  

Thinking about this, however, you use this information to move between the tasks
within a given type of data.
For example, you can use the same data for clustering as you did for classification.
Switching the task changes the questions though: classification evaluation tells
us how separable the classes are given that classifiers decision rule. Clustering
can find other subgroups or the same ones, so the evaluation we choose allows us
to explore this in more ways.



Regression requires a continuous target, so we need a dataset to be suitable for
that, we can't transform from the classification dataset to a regression one.  
However, we can go the other way and that's how some classification datasets are
created.



The UCI [adult](https://archive.ics.uci.edu/ml/datasets/adult) Dataset is a popular ML dataset that was dervied from census
data. The goal is to use a variety of features to predict if a person makes
more than $50k per year or not. While income is a continuous value, they applied
a threshold ($50k) to it to make a binary variable. The dataset does not include
income in dollars, only the binary indicator.  


Recent work reconstructed the dataset with the continuous valued income.
Their [repository](https://github.com/zykls/folktables) contains the data as well
as links to their paper and a video of their talk on it.


## Cross Validation

This week our goal is to learn how to optmize models. The first step in that is
to get a good estimate of its performance.  

We have seen that the test train splits, which are random, influence the
performance.

In order to find the best {term}`hyperparameter`s for the model on a given dataset, we need some better evaluation techniques. 

```{code-cell} ipython3
# basic libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# models classes
from sklearn import tree
from sklearn import cluster
from sklearn import svm
# datasets
from sklearn import datasets
# model selection tools
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold, ShuffleSplit
from sklearn import metrics
```

This week and next week together are about model selection or choosing what model we might want to actually *use*

### Loading Data with sklearn
:::{note}
in  class we tried to use `as_frame=True` alone, but I had misread. 
To figure it out, I looked at the [source code](https://github.com/scikit-learn/scikit-learn/blame/6e9039160f0dfc3153643143af4cfdca941d2045/sklearn/datasets/_base.py#L726-L729) to see what it does, I saw that it does not return the structure for that case, but it does call it.  This is also in the git blame view where I could see it had not changed recently. 
What it does is change the *type* of those attributes in the bunch or the two items returned if in the `return_X_y`, so here.  

It does say that but is an easy thing to misinterpret.  This is the advantage of open source code though, worst case, even if the documentation confuses you, you can see the actual code and see it. 
:::

This way we can get dataFrames:
```{code-cell} ipython3
iris_X, iris_y = datasets.load_iris(return_X_y=True,as_frame=True)
type(iris_X)
```

or in the bunch
```{code-cell} ipython3
iris_bunch_frame= datasets.load_iris(as_frame=True)
type(iris_bunch.data)
```

otherwise the bunch contains numpy arrays. 
```{code-cell} ipython3
iris_bunch= datasets.load_iris()
type(iris_bunch.data)
```

### K-fold Cross Validation
We'll use the Iris data with a decision tree.

We are going to use it this way for claass:

```{code-cell} ipython3
iris_X, iris_y = datasets.load_iris(return_X_y=True)
```

and instantiate a Decision tree [see classificaiton notes about dt](dtintro)


```{code-cell} ipython3
dt = tree.DecisionTreeClassifier()
```


We can split the data, fit the model, then compute a score, but since the
splitting is a randomized step, the score is a random variable.


For example, if we have a coin that we want to see if it's fair or not. We would
flip it to test.  One flip doesn't tell us, but if we flip it a few times, we
can estimate the probability it is heads by counting how many of the flips are
heads and dividing by how many flips.  


We can do something similar with our model performance. We can split the data
a bunch of times and compute the score each time.

`cross_val_score` does this all for us.


It takes an estimator object and the data.

By default it uses 5-fold cross validation. It splits the data into 5 sections,
then uses 4 of them to train and one to test. It then iterates through so that
each section gets used for testing.

```{code-cell} ipython3
cross_val_score(dt, iris_X, iris_y)
```
We get back a score for each section or "fold" of the data. We can average those
to get a single estimate.

To actually report, we would take the mean

::::{margin}
:::{note}
this is not the mean of the numbers from the cell above, because when 
we call the function again, it does another round of random splits
:::
::::

```{code-cell} ipython3
np.mean(cross_val_score(dt, iris_X, iris_y))
```

We can change from 5-fold to 10-fold by setting the `cv` parameter

```{code-cell} ipython3
cross_val_score(dt, iris_X, iris_y,cv=10)
```

In K-fold cross validation, we split the data in `K` sections and train on `K-1` and test one 1 section. So the percentages are like:
```{code-cell} ipython3
# K is the cv
K = 10
train_pct =(K-1)/K
test_pct = 1/K
train_pct, test_pct
```

## Cross validation is model-agnostic


We can use *any* estimator object here.

For example, we can apply it to clustering too:

```{code-cell} ipython3
km = cluster.KMeans(n_clusters = 3)
```

```{code-cell} ipython3
cross_val_score(km, iris_X)
```

## Details and types of Cross validation

First, lets look at the Kfold cross validation object. Under the hood, this is the default thing that `cross_val_score` uses [^cvsstrat]

[^cvsstrat]: actually above, since we used a classifier it actually used stratified cross validaiton whichh solves the problem we discussed in class where the data as sorted and that could be misleading.  [StratifiedKfold](https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold) makes each fold (section) have the same percentage of each class for the classifier.  It only does this if the estimator is a classifier, for clustering and regression it always does regular Kfold.


Cross validation objects are all examples of the [`splitter`](https://scikit-learn.org/dev/glossary.html#term-CV-splitter) type in `sklearn` 

we can instantiate one of these too: 

```{code-cell} ipython3
kf = KFold(n_splits=10)
```

it returns the splits as a generator

::::{margin}
:::{note}
Generators return an item each time you call it in a list, instead of
building the whole list up front, which makes it more memory efficient.

the Python wiki has a good [generator example](https://wiki.python.org/moin/Generators) and the official docs have a more technical [description](https://docs.python.org/3/reference/expressions.html#generator-iterator-methods)
:::
::::


```{code-cell} ipython3
splits = kf.split(iris_X, iris_y)
splits
```

jupyter does not know how to display it, so it jsut says the type. 

```{code-cell} ipython3
type(splits)
```

```{code-cell} ipython3
type(range(3))
```


We can use this in a loop to get the list of indices that will be used to get the test and train data for each fold.  To visualize what this is  doing, see below.

::::{warning}
I changed this to create an empty list and append a dataframe to it each time through the list then create a combined dataframe after to remove the 
::::

```{code-cell} ipython3
N_samples = len(iris_y)
kf_tt_list = []
i = 1
for train_idx, test_idx in splits:
    
    # make a list of "unused"
    col_data =  np.asarray(['unused']*N_samples)
    # fill in train and test by the indices
    col_data[train_idx] = 'Train'
    col_data[test_idx] = 'Test'
    kf_tt_list.append(pd.DataFrame(data = col_data,index=list(range(N_samples)), columns = ['split ' + str(i)]))
    i +=1

kf_tt_df = pd.concat(kf_tt_list,axis=1)
```

Note that unlike [`test_train_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) this does not always randomize and shuffle the data before splitting.

```{code-cell} ipython3
kf_tt_df
```
We can also visualize:

```{code-cell} ipython3
cmap = sns.color_palette("tab10",10)
g = sns.heatmap(kf_tt_df.replace({'Test':1,'Train':0}),cmap=cmap[7:9],cbar_kws={'ticks':[.25,.75]},linewidths=0,
    linecolor='gray')
colorbar = g.collections[0].colorbar
colorbar.set_ticklabels(['Train','Test'])
```

We can also see another splitter(this is what `train_test_split` uses)

```{code-cell} ipython3
skf = ShuffleSplit(10)
N_samples = len(iris_y)
ss_tt_list = []
i = 1
for train_idx, test_idx in skf.split(iris_X, iris_y):
    col_data = np.asarray(['unused']*N_samples)
    col_data[train_idx] = 'Train'
    col_data[test_idx] = 'Test'
    ss_tt_list.append(pd.DataFrame(data = col_data, index=list(range(N_samples))), columns=['split' + str(i)])
    i +=1

ss_tt_df = pd.concat(ss_tt_list,axis=1)
ss_tt_df
```

```{code-cell} ipython3
cmap = sns.color_palette("tab10",10)
g = sns.heatmap(ss_tt_df.replace({'Test':1,'Train':0}),cmap=cmap[7:9],cbar_kws={'ticks':[.25,.75]},linewidths=0,
    linecolor='gray')
colorbar = g.collections[0].colorbar
colorbar.set_ticklabels(['Train','Test'])
```

we can see this is different and more random.

We can also use it in `cross_val_score`

```{code-cell} ipython3
cross_val_score(dt,iris_X,iris_y,cv=ShuffleSplit(n_splits=100,test_size=.2))
```

```{code-cell} ipython3

```

## Questions after class

### When should you change test and train sizes rather than just using the default?

Having more training data often gets you a better model, having more test samples gives you a better idea of how well the model will work on new data.  You have to make that choice.  

I think sometimes trying a few and seeing if the performance changes much or not, can also help you decide, if it doesn't matter much, then use the default. If it changes a lot, then maybe use as much data as possible for training and you can go as far as KFold cross validation with `K = len(X)` which is called leave-one-out cross validation. 

