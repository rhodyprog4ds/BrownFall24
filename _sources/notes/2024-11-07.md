---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.16.4
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

# Model Optimization

```{important}
Remember that best is context dependent and relative.  The best accuracy might not be the best overall. Automatic optimization can only find the best thing in terms of a single score.
```

```{code-cell} ipython3
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd
from sklearn import datasets
from sklearn import cluster
from sklearn import svm, datasets
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn import tree
```

## Model validation
We will work with the iris data again.

```{code-cell} ipython3
iris_df = sns.load_dataset('iris')


iris_X = iris_df.drop(columns=['species'])

iris_y = iris_df['species']
```

We will still use the test train split to keep our test data separate from the data that we use to find our preferred parameters.

```{code-cell} ipython3
iris_X_train, iris_X_test, iris_y_train, iris_y_test = train_test_split(iris_X,iris_y,
                                                                        random_state=0)
```

````{margin}
```{admonition} Further Reading
the criteria are discussed [in the mathematical formulation](https://scikit-learn.org/stable/modules/tree.html#mathematical-formulation) of the sklearn documentation
```
````

Today we will optimize a decision tree over three parameters. One is the criterion, which is how it decides where to create thresholds in parameters. Gini is the default and it computes how concentrated each class is at that node, another is entropy, entropy is, generally how random something is.  Intuitively these do similar things, which makes sense because they are two ways to make the same choice, but they have slightly different calculations.

The other two parameters relate to the structure of the decision tree that is produced. 
- Max depth is the height of the tree
- min smaples per leaf makes it keeps the leaf sizes small.

```{code-cell} ipython3
dt = tree.DecisionTreeClassifier()
params_dt = {'criterion':['gini','entropy'],'max_depth':[2,3,4],
       'min_samples_leaf':list(range(2,20,2))}
```

We will fit it with default CV settings.

```{code-cell} ipython3
dt_opt = GridSearchCV(dt,params_dt)
```

then we can fit

```{code-cell} ipython3
dt_opt.fit(iris_X_train,iris_y_train)
```

we can use ti to get predictions

```{code-cell} ipython3
y_pred = dt_opt.predict(iris_X_test)
```

we can also score it as regular.

```{code-cell} ipython3
dt_opt.score(iris_X_test,iris_y_test)
```

we can also see the best parameters.

```{code-cell} ipython3
dt_opt.best_params_
```

we can look at the overall results this way:

```{code-cell} ipython3
dt_5cv_df = pd.DataFrame(dt_opt.cv_results_)
dt_5cv_df.head()
```

To see more carefully what it does, we can look at its shape.

```{code-cell} ipython3
dt_5cv_df.shape
```

we can see that the total number of rows matched the product of the length of each list of parameters to try.

```{code-cell} ipython3
np.product([len(param_list) for param_list in params_dt.values()])
```

This means that it tests every combination of features-- without us writing a bunch of nested loops.

We can also plot the dta and look at the performance.

```{code-cell} ipython3
sns.catplot(data=dt_5cv_df,x='param_min_samples_leaf',y='mean_test_score',
           col='param_criterion', row= 'param_max_depth', kind='bar',)
```

this makes it clear that none of these stick out much in terms of performance.



## Impact of CV parameters

```{code-cell} ipython3
dt_opt10 = GridSearchCV(dt,params_dt,cv=10)
dt_opt10.fit(iris_X_train,iris_y_train)
```

```{code-cell} ipython3
dt_10cv_df = pd.DataFrame(dt_opt10.cv_results_)
```

We can stack the columns we want from the two results together with a new indicator column `cv`

```{code-cell} ipython3
plot_cols = ['param_min_samples_leaf','std_test_score','mean_test_score',
             'param_criterion','param_max_depth','cv']
dt_10cv_df['cv'] = 10
dt_5cv_df['cv'] = 5

dt_cv_df = pd.concat([dt_5cv_df[plot_cols],dt_10cv_df[plot_cols]])
dt_cv_df.head()
```

this can be used to plot.

```{code-cell} ipython3
sns.catplot(data=dt_cv_df,x='param_min_samples_leaf',y='mean_test_score',
           col='param_criterion', row= 'param_max_depth', kind='bar',
           hue = 'cv')
```

we see that the mean scores are not very different, but that 10 is a little higher in some cases.  This makes sense, it has more data to learn from, so it found something that applied better, on average, to the test set.

```{code-cell} ipython3
sns.catplot(data=dt_cv_df,x='param_min_samples_leaf',y='std_test_score',
           col='param_criterion', row= 'param_max_depth', kind='bar',
           hue = 'cv')
```

However here we see that the variabilty in those scores is much higher, so maybe the 5 is better.

We can compare to see if it finds the same model as best:

```{code-cell} ipython3
dt_opt.best_params_
```

```{code-cell} ipython3
dt_opt10.best_params_
```

In some cases they will and others they will not.

```{code-cell} ipython3
dt_opt.score(iris_X_test,iris_y_test)
```

```{code-cell} ipython3
dt_opt10.score(iris_X_test,iris_y_test)
```

In some cases they will find the same model and score the same, but it other time they will not.

The takeaway is that the cross validation parameters impact our ability to measure the score and possibly how close that cross validation mean score will match the true test score. Mostly it will change the variability in the estimate of the score.  It does not change necessarily which model is best, that is up to the data iteself (the original test/train split would impact this).

```{admonition} Try it yourself
Does this vary if you repeat this with different test sets? How much does it depend on that? Does repeating it produce the same scores? Does one take longer to fit or score?
```



## Other searches

```{code-cell} ipython3
from sklearn import model_selection
from sklearn.model_selection import LeaveOneOut
```

```{code-cell} ipython3
rand_opt = model_selection.RandomizedSearchCV(dt,params_dt).fit(iris_X_train,
                                                     iris_y_train)
```

```{code-cell} ipython3
rand_opt.score(iris_X_test,iris_y_test)
```

```{code-cell} ipython3
rand_opt.best_params_
```

It might find the same solution, but it also might not.  If you do some and see that the parameters overall do not impact the scores much, then you can trust whichever one, or consider other criteria to choose the best model to use.

```{code-cell} ipython3

```
