---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.16.4
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

# Classifying Text


```{code-cell} ipython3
from sklearn.feature_extraction import text
from sklearn.metrics.pairwise import euclidean_distances
from sklearn import datasets
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.naive_bayes import MultinomialNB, GaussianNB
from sklearn.model_selection import train_test_split
# edited so I can reuse these later
selected_categories = ['comp.graphics','sci.crypt']
ng_X,ng_y = datasets.fetch_20newsgroups(categories =selected_categories,
                                       return_X_y = True)

from sklearn.metrics import confusion_matrix
```

## review of count representation

```{code-cell} ipython3
ex_vocab = ['and', 'are', 'cat', 'cats', 'dogs', 'pets', 'popular', 'videos']
sample = 'Cats and dogs are pets'
rep = [1, 1, 0, 1, 1, 1, 0, 0]
```

## News Data

Lables are the topic of the article 0 = computer graphics, 1 = cyrptography.

```{code-cell} ipython3
type(ng_X)
```

```{code-cell} ipython3
len(ng_X)
```

the X is he actual text
```{code-cell} ipython3
:tags: ["scroll-output"]
print(ng_X[0])
```

```{code-cell} ipython3
ng_y[:3]
```

## Count Vectorization
We're going to instantiate the object and fit it two the whole dataset.
```{code-cell} ipython3
count_vec = text.CountVectorizer()
ng_vec = count_vec.fit_transform(ng_X)
```

```{code-cell} ipython3
ng_vec.shape
```

```{code-cell} ipython3
type(ng_vec)
```

Now get a classifier ready:

```{code-cell} ipython3
clf = MultinomialNB()
```

THen train/test split:

```{code-cell} ipython3
ng_vec_train, ng_vec_test, ng_y_train, ng_y_test = train_test_split(ng_vec,ng_y)
```

```{code-cell} ipython3
clf.fit(ng_vec_train,ng_y_train).score(ng_vec_test,ng_y_test)
```

```{code-cell} ipython3
clf.__dict__
```

We can predict on new articles and by transforming and then passing it to our classifierr. 
```{code-cell} ipython3
article_vec = count_vec.transform(['this is about cryptography'])
clf.predict(article_vec)
```

We can see that it was confident in that prediction:
```{code-cell} ipython3
clf.predict_proba(article_vec)
```

It would be high the other way with a different sentence:

```{code-cell} ipython3
article_vec = count_vec.transform(['this is about image processing'])
clf.predict(article_vec)
```

If we make something about both topics, we  can get less certain predictions:

```{code-cell} ipython3
article_vec = count_vec.transform(['this is about encrypting images'])
clf.predict_proba(article_vec)
```

## TF-IDF

This stands for term-frequency inverse document frequency. for a document number $d$ and word number $t$ with $D$ total documents:

$$\operatorname{tf-idf(t,d)}=\operatorname{tf(t,d)} \times \operatorname{idf(t)}$$

where:
$$\operatorname{idf}(t) = \log{\frac{1 + n}{1+\operatorname{df}(t)}} + 1$$

and
- $\operatorname{df}(t)$ is the number of documents word $t$ occurs in
- $\operatorname{tf(t,d)}$ is te number of times word $t$ occurs in document $d$



then `sklearn` also normalizes as follows:

$$v_{norm} = \frac{v}{||v||_2} = \frac{v}{\sqrt{v{_1}^2 +
v{_2}^2 + \dots + v{_n}^2}}$$

```{code-cell} ipython3
tfidf = text.TfidfTransformer()
ng_tfidf = tfidf.fit_transform(ng_vec)
```


## Comparing representations

+++
Which representation is better?

To to this, we resplit the data so that the same sampls are in the test set for both representations:

```{code-cell} ipython3
ng_vec_train, ng_vec_test, ng_tfidf_train, ng_tfidf_test,ng_y_train, ng_y_test = train_test_split(ng_vec, ng_tfidf, ng_y)
```

::::{warning}
In class, i used Multinomial NB for both and verbally said that was a bad choice because the TFIDF does not meet the assumptions it is not counts.  

I have switched this to Gaussian. 
::::
Now we get two new classifiers
```{code-cell} ipython3
clf_vec = MultinomialNB()
clf_tfidf = GaussianNB()
```

and fit and score each
```{code-cell} ipython3
clf_vec.fit(ng_vec_train,ng_y_train).score(ng_vec_test,ng_y_test)
```

```{code-cell} ipython3
clf_tfidf.fit(ng_tfidf_train,ng_y_train).score(ng_tfidf_test,ng_y_test)
```

To get a more robust answer, we should use cross validation and likely use a different classifier.  Multinomal Naive Bayes is well suited for the count data, but, strictly speaking, not the normalized ones. 



## Distances in Text Data

```{code-cell} ipython3
ng_y[:20]
```

```{code-cell} ipython3
selected_categories
```

Next we will take a few samples of each and see what the numerical representation gives us

```{code-cell} ipython3
first_graphics = np.where(ng_y == 0)[0]
first_crypt = np.where(ng_y == 1)[0]
```

`where` returns the indices where the condition is true

```{code-cell} ipython3
subset_rows = np.concatenate([first_graphics,first_crypt])
subset_rows
```

```{code-cell} ipython3
sns.heatmap( euclidean_distances(ng_tfidf[subset_rows]))
```

This shows that the graphics articles are close to one another, but the cryptography articles are far apart. 

This mean thatt the graphics article are more similar to one another than they are to the cryptography articles.  It also shows that the cryptography articles are more diverse than the graphics articles. 

+++

We could also look att only the test samples, but we can sort them

```{code-cell} ipython3
test_sorted = np.concatenate([np.where(ng_y_test == 0)[0],np.where(ng_y_test == 1)[0]])
sns.heatmap( euclidean_distances(ng_tfidf_test[test_sorted]))
```

```{code-cell} ipython3
y_pred_tfidf = clf_tfidf.predict(ng_tfidf_test)
cols = pd.MultiIndex.from_arrays([['actual class']*len(selected_categories),
                                  selected_categories])
rows = pd.MultiIndex.from_arrays([['predicted class']*len(selected_categories),
                                  selected_categories])

pd.DataFrame(data = confusion_matrix(y_pred_tfidf,ng_y_test),columns = cols,
             index = rows)
```

This makes sense that more of the errors are mislabeling computer graphics as cryptography, since the graphics articles are more diers and the cryptography articles were mostly similar to one another. 

```{code-cell} ipython3

```
