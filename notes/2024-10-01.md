---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.16.4
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

# Webscraping 

```{code-cell} ipython3
import requests
from bs4 import BeautifulSoup
import pandas as pd
```


````{warning}
If it says it cannot load one of the libraries, use pip inside your notebook to install, 

```
pip install beautifulsoup4
```

then restart your kernel (Kernel menu, choose restart)
````



## Getting Data From Websites 


We have seen that `read_html` can get content from an actual website, not a data file that is hosted somewhere on the internet, that takes tables on a website and returns a list of DataFrames.

````{margin}
```{note}
This has a long output, so I hid it by default, but you can view it
```
````

```{code-cell} ipython3
:tags: ["hide-output"]
pd.read_html('https://rhodyprog4ds.github.io/BrownSpring23/syllabus/achievements.html')
```

This gives us a list of DataFrames that come from the website.  `pandas` gets tables by looking in the html for the site and finding the `<table>` tags. 

If we store it in a variable , we can see that 
```{code-cell} ipython3
df_list = pd.read_html('https://rhodyprog4ds.github.io/BrownFall24/syllabus/achievements.html')
[type(d) for d in df_list]
```

## Everything is Data

For the purpose of this class, it is best to think of the content on a web page like a datastructure.  

![html anatomy](https://cdn2.hubspot.net/hub/53/file-1519188549-jpg/blog-files/webpage-setup.jpg)


![HTML tree structure](http://www.w3schools.com/js/pic_htmltree.gif)

there are tags `<>` that define the structure, and these can be further classified with `classes`



## Scraping a URI website


We're going to create a DataFrame about URI CS & Statistics Faculty.

from the [people page](https://web.uri.edu/cs/people/) of the department website.

````{margin}
```{note}
the inspect link goes to instructions for different browsers
```
````
We can [inspect](https://blog.hubspot.com/website/how-to-inspect) the page to check that it's well structured.  

```{warning}
With great power comes great responsibility.

- always check the [robots.txt](https://web.uri.edu/robots.txt)
- do not do things that the owner says not to do
- government websites are typically safe
```


We'll save the URL for easy use



```{code-cell} ipython3
cs_people_url  = 'https://web.uri.edu/cs/people/'
```

Then we can use the `requests` library to make a call to the internet.  It actually gets back a [response object](https://requests.readthedocs.io/en/latest/api/#requests.Response) which has a lot of extra information.  For today we only need the `content` from the page which is an attrtibute of that object: 

```{code-cell} ipython3
cs_people_html = requests.get(cs_people_url).content

cs_people = BeautifulSoup(cs_people_html,'html.parser')
```

This is raw: 
````{margin}
```{note}
here I suppressed the output in class by looking only at the first few characters
```
````
```{code-cell} ipython3
cs_people_html[:100]
```


```{literalinclude} https://web.uri.edu/cs/people/
:start-at: Department Chair
:end-before: Directors
:lineno-match:
```


But we do not need to manually write search tools, that's what [`BeautifulSoup`](https://beautiful-soup-4.readthedocs.io/en/latest/) is for.

```{code-cell} ipython3
:tags: ["scroll-output"]
cs_people
```


```{code-cell} ipython3
type(cs_people)
```

### Looking at tags 

In this object we can use any tag from the file and get back the first instance

the [`<a>`](https://www.w3schools.com/tags/tag_a.asp) tag in HTML makes a link

```{code-cell} ipython3
cs_people.a
```

````{tip}
This content is a [best practice](https://www.a11y-collective.com/blog/skip-to-main-content/#:~:text=smooth%20transition%20effect.-,Best%20practices%20for%20%E2%80%98Skip%20to%20main%20content%E2%80%99%20links,-Incorporating%20%E2%80%98Skip%20navigation) for accessible web design
````

We also see `<h3>` in the code so we can get the first one like this: 

```{code-cell} ipython3
cs_people.h3
```


this [cheatsheet](https://web.stanford.edu/group/csp/cs21/htmlcheatsheet.pdf) shows lots of html tags, but for this purpose you do not really need it. You'll be inspecting the page and then looking for what you want

### Searching the source


More helpful is the `find_all` method we want to find all `div` tags that are "peopleitem" class. We decided this by inspecting the code on the website.

```{code-cell} ipython3
type(cs_people.find_all('div','peopleitem'))
```

```{code-cell} ipython3
:tags: ["scroll-output"]
cs_people.find_all('div','peopleitem')
```


this is a long, object and we can see it looks iterable (`[` at the start)

```{code-cell} ipython3
people_items = cs_people.find_all('div','peopleitem')

```

We can look at a single item
```{code-cell} ipython3
:tags: ["scroll-output"]
people_items[0]
```

and how many it finds

```{code-cell} ipython3
len(people_items)
```


```{important}
answer to questions about searching [from the docs](https://beautiful-soup-4.readthedocs.io/en/latest/index.html?highlight=find_all#the-keyword-arguments)
```

We notice that the name is inside a `<h3>` tag with class `p-name` and then inside an a tag after that.  We also know from looking at the overall page that there are lots of other a tags, so we do not want to search all of those.

```{code-cell} ipython3
people_items[0].find('h3','p-name')
```
Then we see in this, there is an `<a>` tag, so we can pull that out next, we can use the tag attribute, because the first instance of the tag is exactly what we want. 
```{code-cell} ipython3
people_items[0].find('h3','p-name').a
```
inside that there is the text in a string, so we can pull that out 
```{code-cell} ipython3
people_items[0].find('h3','p-name').a.string
```


````{margin}
```{important}
In the course notes, I repeat very similar cells so that the notes replicate the *thinking* process to do the analysis. In your assignments, you do not need to include intermediate steps, there the target audience is a person interested in the conclusions where here, as students, *you* are the target audience. 
```
````

Finally, now that we know how to get one out, we can put it all in a list comprehension


```{code-cell} ipython3
names = [person.find('h3','p-name').a.string for person in people_items]
```

pull out the titles for each person  and store in a variable `titles`

## Pulling more information

First, we look at the whole person entry again. 

```{code-cell} ipython3
people_items[0]
```
on one item, the `p` tag with the `people-title` class gets us what we want and
then we can 
```{code-cell} ipython3
title = [person.find('p','people-title').string for person in people_items]
```

We can also use the whole page object, but in some cases it might be safer or faster to only work with the chunk we had separate (like above with `people_items`).  There
are many ways and all are valid, but it is worth thinking about pros and cons. 
If there were any of the thing we were searching for in a different part that we did not want here, then the subset would be  more accurate. 

We can pull out two more things, the people-department indicates who is CS & who is Statistics.
```{code-cell} ipython3
disciplines = [d.string for d in cs_people.find_all("p",'people-department')]
emails = [e.string for e in cs_people.find_all("a",'u-email')]
```


We can finally use the DataFrame constructor to make it a table.  I chose to use a dictionary in class
```{code-cell} ipython3
css_df = pd.DataFrame({'name':names,'title':title,
              'discipline':disciplines, 'email':emails})
css_df.head()
```


We could also use a list of list and separate list of column names. 
```{code-cell} ipython3
css_df_b = pd.DataFrame(data=[names,titles,emails,disciplines],
                       columns =['names','titles','emails','disciplines'])
css_df_b
```


## Crawling and scraping

Remember we pulled the names out of links, when in the browser, we click on the links, we see that they are to a profile page. On these pages, they have the office number.  Let's add those to our dataframe. 

First, we will do it for one person, then make a loop. 


```{code-cell} ipython3
people_items[0].find('h3','p-name').a
```

We see that the information that we want is in the `href` attribute, to read that, we check the [documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#attributes). This tells us there is a `.attrs` attribute of the python object we are working with. 

```{code-cell} ipython3
people_items[0].find('h3','p-name').a.attrs
```
the classes are allso attributes in HTML so we could access that informaiton this way: 
```{code-cell} ipython3
people_items[0].find('h3','p-name').attrs
```
It's a dictionary and the attribute we want is the key we want. 

```{code-cell} ipython3
chair_url = people_items[0].find('h3','p-name').a.attrs['href']
chair_url
```

Now, we do the same thing we did above, request, pull the content from the response and then use the parser. 

```{code-cell} ipython3

chair_html = requests.get(chair_url).content
chair_info = BeautifulSoup(chair_html,'html.parser')
```
then we find the tag and class we need from inspecting and pull that. 

```{code-cell} ipython3
chair_info.find('li','people-location')
```
This structure is different and we tried string without success


Here, we could go to the documentation and look up what the object contains, or, we can use object serialization.  
We can use the python [`__dict__`](https://docs.python.org/3/reference/datamodel.html#object.__dict__) to inspect the object and see where it stored what we want.  


```{code-cell} ipython3
chair_info.find('li','people-location').__dict__
```
we see its the second element in a list in the `'content'` value
```{code-cell} ipython3
chair_info.find('li','people-location').contents
```
so we can pull it out 
```{code-cell} ipython3
chair_info.find('li','people-location').contents[1]
```

Now tht we know how to do it, we can put it in a loop.  
```{code-cell} ipython3
offices = []
for person in people_items:
    person_url = person.find('h3','p-name').a.attrs['href']
    
    person_html = requests.get(person_url).content
    person_info = BeautifulSoup(person_html,'html.parser')
    try: 
        office_loc = person_info.find('li','people-location').contents[1]
        offices.append(office_loc)
    except:
        offices.append(pd.NA)
```



We added the [`try` and `except`](https://docs.python.org/3/tutorial/errors.html#handling-exceptions) to handle when there is no office location. This is something in practice you would often think to do due an error.  

Next we add the offices to our dataframe: 

```{code-cell} ipython3
css_df['offices'] = offices
```
and peak at the head
```{code-cell} ipython3
css_df.head()
```

Here we check the `info()` and we can see many were missing. 


```{code-cell} ipython3
css_df.info()
```

## Questions

```{note}
Some of these are questions asked in previous semesters
```

### what does .a do?
it gives the first instance of the `<a>` tag 

### is it worth it to try and web scrape a page that is poorly written?

If it is important information.  In these cases, you might have to do more manual parsing or even some manual fixes. 

For this class, no. 

### Is there a way to check robots.txt through BeautifulSoup or must that be done manually in a browser?

it could maybe be read programmaticlaly, but it doesn't necessarily save time to do it that way. 

### What else can I do with inspect?

It lets you view the code.  It's most often used to debug websites. 

### when web scraping if the html is not set up well is it possible to change the html to make it easier to parse

Technically you could manually edit a copy of it. 

### Are there instances where you can get data from websites that are not in tabular form?

Web scraping is *for* when the website is not in tabular form.  It should be strucutred, but the structure does not need to come from a single page.  It could be that there are many pages strucutred similarly and you build most of the columns from the other pages, not the starting page. 

For example from the [teams page of the nba](https://www.nba.com/teams) you can get to a page with info about each team that includes all time records and the current rosters. On these individual pages, most info is an actual table, so you can use `pd.read_html` for those, but the crawing part from the first page would count. 
